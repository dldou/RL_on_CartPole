{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning\n",
    "\n",
    "References:  \n",
    "[1] Hands-On Machine Learning, Aurélien Géron, 3rd edition, O'reilly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few notions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforcement Learning definition**:  \n",
    "A software agent makes observations and takes actions within an environnment and in return it receives a rewards from the environnment. Its objective is to learn to act in a way that will maximize its expected reward over time.\n",
    "\n",
    "**Policy**: The function the agent uses to determine its actions $\\;\\;\\Pi : O \\rightarrow A$. When this policy involves randomness we call it stochastic policy.  \n",
    "            - *Grid search policy*: looking for the best settings of the parameters by exploring a grid of values.  \n",
    "            - *Bruce force approch search policy*: The agents tries random values of parameters and choose the best setting.  \n",
    "\n",
    "**Policy Gradients** is an algorithm used to update the policy function. It optimizes the parameters of a policy by following the gradients towards higher rewards. Here's one common variant of Reinforce algorithms:  \n",
    "> 1) The net plays several games and at each steps compute the gradients of the rewards\n",
    "> 2) Once several games have been run, compute each action's advantage \n",
    "> 3) If action has a postive action's advantage we'd like the agent to choose the action more likely meaning we'd like to apply apply the gradients whereas if the action's advantage is negative, we'd like to apply the opposite of the gradients. Each gradient vector is multiplied by the corresponding action's advantage.\n",
    "> 4) Compute the mean of all resulting gradient vectors and use it to perform the gradient descent step.\n",
    "\n",
    "**Evaluating actions**: As the reward is delayed, the problem cannot boils down to a simple supervised learning. The credit assignement problem is the difficulty to link which actions caused the good or bad reward from the environnment. To tackle this problem we consider the sum of all future rewards after that the action has been taken $\\sum_{i=t}^{n} \\gamma^{i-t} \\times r_i$ it is used with a discount factor $\\gamma$. The best way to interprete the discount factor is to find at which index $k$ $\\gamma^k$ equals $0.5$.  \n",
    "To evaluate the *action's advantage* we must run many episodes and normalize all actions returns. Action with positive action advantage were good whereas actions with negative ones were bad.  \n",
    "\n",
    "**Other methods**:   \n",
    "As it might exists a lot of states and actions, the policy might be really complicated therefore we might choose an algorithm that learns to estimate the expected return for each state or each action in each state and use this knowlegde to learn how to act."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs description\n",
      "horizontal position: 0.02739560417830944\n",
      "horizontal velocity: -0.006112155970185995\n",
      "angle: 0.03585979342460632\n",
      "angular rotation: 0.019736802205443382\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
      "1.0\n",
      "False\n",
      "False\n",
      "{}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make (\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Init the env\n",
    "obs, info = env.reset(seed=42)\n",
    "print(f\"Obs description\\nhorizontal position: {obs[0]}\\nhorizontal velocity: {obs[1]}\\nangle: {obs[2]}\\nangular rotation: {obs[3]}\")\n",
    "\n",
    "# Show the state\n",
    "img_init_state = env.render()\n",
    "plt.imshow(img_init_state)\n",
    "plt.show()\n",
    "\n",
    "# Show available actions (left/right)\n",
    "env.action_space \n",
    "\n",
    "# Example of action\n",
    "action = 1\n",
    "obs, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "print(obs)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(truncated)\n",
    "print(info)\n",
    "\n",
    "img_state = env.render()\n",
    "plt.imshow(img_state)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic and deterministic policy \n",
    "We will try the following policy: when the pole is titled toward the right, the cart go right and when the pole is titled toward the left we go left. \n",
    "$$\n",
    "\\Pi : O \\rightarrow A \\\\\n",
    "\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; o \\rightarrow 1 \\text{\\;if angle > 0 and\\;} 0 \\text{\\;else} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_deterministic_policy(obs):\n",
    "    if obs[2] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of the policy\n",
    "def run_episodes(n_episodes, n_max_steps, policy):\n",
    "    \n",
    "    rewards = list()\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "\n",
    "        running_reward = 0\n",
    "        obs, info = env.reset(seed=episode)\n",
    "\n",
    "        for step in range(n_max_steps):\n",
    "\n",
    "            action = policy(obs)\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            running_reward += reward\n",
    "\n",
    "            if truncated or done:\n",
    "                break \n",
    "\n",
    "        rewards.append(running_reward)\n",
    "        env.close()\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic on results\n",
      "Mean reward: 41.698\n",
      "Median reward: 40.0\n",
      "Std: 8.39\n",
      "Max reward: 63.0\n"
     ]
    }
   ],
   "source": [
    "n_episodes  = 500 \n",
    "n_max_steps = 200\n",
    "rewards_list = run_episodes(n_episodes, n_max_steps, simple_deterministic_policy)\n",
    "\n",
    "# Print info\n",
    "print(f\"Statistic on results\\nMean reward: {np.mean(rewards_list)}\\nMedian reward: {np.median(rewards_list)}\\nStd: {np.std(rewards_list):.2f}\\nMax reward: {np.max(rewards_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import cv2 as cv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check video for one episode \n",
    "def add_episode_label_on_img(frame, episode_num):\n",
    "\n",
    "    if np.mean(frame) < 128:\n",
    "        text_color = (255,255,255)\n",
    "    else:\n",
    "        text_color = (0,0,0)\n",
    "    \n",
    "    cv.putText(frame, f'Episode: {episode_num+1}', (frame.shape[0]//20, frame.shape[1]//18), cv.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def add_reward_label_on_img(frame, reward):\n",
    "\n",
    "    if np.mean(frame) < 128:\n",
    "        text_color = (255,255,255)\n",
    "    else:\n",
    "        text_color = (0,0,0)\n",
    "    \n",
    "    cv.putText(frame, f'REWARD: {reward}', (frame.shape[0] - frame.shape[0]//3 - 10, frame.shape[1] - frame.shape[1]//3 - 10), cv.FONT_HERSHEY_SIMPLEX, 1, text_color, 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "def get_episodes_frames(env, n_episodes, n_max_steps, policy):\n",
    "\n",
    "    frames_all_episodes = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "\n",
    "        obs, _ = env.reset(seed=episode)\n",
    "        running_reward = 0\n",
    "\n",
    "        # Collect init image\n",
    "        init_frame = env.render()\n",
    "        add_episode_label_on_img(init_frame, episode_num=episode)\n",
    "        frames_all_episodes.append(init_frame)\n",
    "\n",
    "        for step in range(n_max_steps):\n",
    "\n",
    "            action = policy(obs)\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "            # Collect image\n",
    "            frame = env.render()\n",
    "            add_episode_label_on_img(frame, episode)\n",
    "            frames_all_episodes.append(frame)\n",
    "\n",
    "            running_reward += reward\n",
    "\n",
    "            if truncated or done:\n",
    "\n",
    "                last_frame = add_reward_label_on_img(frame, running_reward)\n",
    "\n",
    "                for i in range(20):\n",
    "                    frames_all_episodes.append(last_frame)\n",
    "                    \n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return frames_all_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "env         = gym.make (\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "n_episodes  = 50 \n",
    "n_max_steps = 200\n",
    "\n",
    "#frames = get_episodes_frames(env, n_episodes, n_max_steps, simple_deterministic_policy)\n",
    "#imageio.mimwrite(os.path.join('./videos/', 'basic_policy_agent_50_episodes.gif'), frames, fps=60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network policy\n",
    "\n",
    "**We choose to define a stochastic policy.**  \n",
    "As there are only two actions possible - moving to the left or moving to the right - our network will estimate the probability $p$ of choosing the action of moving to the left.   \n",
    "Let X be the random variable that associates \"go to the left\" to $0$ and \"go to the right\" to $1$. In the way we created our simple regression model, X follows the Bernoulli distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic model\n",
    "\n",
    "Two dense layers:\n",
    "- first takes as an input a tensor or size $4 \\times 1$ (+ biais) and is composed of 5 hidden units\n",
    "- second layer is composed of 1 neuron with sigmoid activation to obtain the probability $p$ of choosing the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_layer, n_hidden_units, n_hidden_layers):\n",
    "        super().__init__()\n",
    "        # don't forget to add try here\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "\n",
    "        model = []\n",
    "        for i in range(self.n_hidden_layers + 1):\n",
    "            # input layer\n",
    "            if i == 0:\n",
    "                model += [nn.Linear(input_layer, n_hidden_units)]\n",
    "                model += [nn.ReLU()]\n",
    "            # hidden layers\n",
    "            elif i > 0 and i < (self.n_hidden_layers):\n",
    "                model += [nn.Linear(n_hidden_units, n_hidden_units)]\n",
    "                model += [nn.ReLU()]\n",
    "            # output_layer\n",
    "            elif i == (self.n_hidden_layers):\n",
    "                model += [nn.Linear(n_hidden_units, 1)]\n",
    "                #model += [nn.Sigmoid()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimpleRegression                         [1, 1]                    --\n",
       "├─Sequential: 1-1                        [1, 1]                    --\n",
       "│    └─Linear: 2-1                       [1, 5]                    25\n",
       "│    └─ReLU: 2-2                         [1, 5]                    --\n",
       "│    └─Linear: 2-3                       [1, 1]                    6\n",
       "==========================================================================================\n",
       "Total params: 31\n",
       "Trainable params: 31\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_reg = SimpleRegression(4,5,1)\n",
    "summary(simple_reg, (1,4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training strategy\n",
    "\n",
    "We'll implement a simple variant of Policy Gradient as given in the introduction of this notebook. We set the hyperparameters as follow:\n",
    "- number of episodes before updating the policy = 10\n",
    "- number of updates = 10\n",
    "- number of maximum steps = 200 (limits imposed by the game)  \n",
    "  \n",
    "We'll test several *discount factors*.  \n",
    "  \n",
    "Our loss function will be the simpliest : binary cross entropy. Our optimizer will be the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_step(env, obs, model, loss_fn, optimizer, device):\n",
    "    \"\"\"\n",
    "        Run one step and add the gradient in the output variables\n",
    "    \"\"\"\n",
    "    grads = []\n",
    "\n",
    "    # Set all grads of the model to 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    obs_tensor = torch.tensor(obs).unsqueeze(0).to(device)\n",
    "\n",
    "    # Check the differenciation\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        left_proba = model(obs_tensor)\n",
    "        #print(\"left_proba: \", nn.Sigmoid()(left_proba))\n",
    "        # Choose False with probability left_proba\n",
    "        action = torch.randn(left_proba.shape, device=device) > left_proba\n",
    "        #print(\"action: \", action)\n",
    "        # if action = 0 then the probability to go right will be one\n",
    "        y_target = torch.ones(left_proba.shape, device=device) - action.float()\n",
    "        #print(\"y_target: \", y_target)\n",
    "        #loss = - left_proba * y_target + (1 - left_proba) * (1 - y_target)\n",
    "        loss = - loss_fn(y_target, left_proba) \n",
    "        #print(\"loss: \", loss)\n",
    "\n",
    "        # Compute the gradients of each parameters\n",
    "        loss.backward()\n",
    "\n",
    "    # Collect all gradients in the model parameters\n",
    "    for param in model.parameters():\n",
    "        grads.append(param.grad.clone())\n",
    "\n",
    "    #print(grads)\n",
    "\n",
    "    obs, reward, done, truncated, info = env.step(int(action.item()))\n",
    "\n",
    "    return obs, reward, done, truncated, info, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes_before_policy_gradient_update(env, n_episodes_per_update, n_max_steps, policy, loss_fn, optimizer, device,\n",
    "                                               generate_GIF=False):\n",
    "\n",
    "    rewards_all_episodes = []\n",
    "    grads_all_episodes   = []\n",
    "\n",
    "    if generate_GIF:\n",
    "        frames = []\n",
    "\n",
    "    for episode in range(n_episodes_per_update):\n",
    "\n",
    "        running_rewards = []\n",
    "        running_grads   = []\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        for step in range(n_max_steps):\n",
    "\n",
    "            obs, reward, done, truncated, info, grads = run_one_step(env, obs, policy, loss_fn, optimizer, device)\n",
    "\n",
    "            running_rewards.append(reward)\n",
    "            running_grads.append(grads)\n",
    "\n",
    "            if generate_GIF:\n",
    "                frame = env.render()\n",
    "                labelled_frame = add_episode_label_on_img(frame, episode)\n",
    "                frames.append(labelled_frame)\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        rewards_all_episodes.append(running_rewards)\n",
    "        grads_all_episodes.append(running_grads)\n",
    "\n",
    "    if generate_GIF:\n",
    "        return rewards_all_episodes, grads_all_episodes, frames\n",
    "    else:\n",
    "        return rewards_all_episodes, grads_all_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    \"\"\"\n",
    "                              N-k \n",
    "       discounted_reward[k] = sum (gamma^j x r^(j+k))\n",
    "                              j=0 \n",
    "\n",
    "        where : k is the index of the k-th reward\n",
    "                r is the reward\n",
    "                gamma is the discount factor\n",
    "                N is the number of rewards (N = len(rewards))\n",
    "    \"\"\"\n",
    "    discounted_rewards = np.array(rewards)\n",
    "\n",
    "    for k in range(len(rewards) - 2, -1, -1):\n",
    "        discounted_rewards[k] += discounted_rewards[k + 1] * discount_factor\n",
    "\n",
    "    return discounted_rewards\n",
    "\n",
    "def normalize_discounted_rewards(rewards_all_batch, discount_factor):\n",
    "    \"\"\"\n",
    "        Compute the discount on all the batch of episodes and stock them in an array \n",
    "        then compute the normalization of it\n",
    "    \"\"\"\n",
    "    discounted_all_batch = [discount_rewards(rewards, discount_factor) for rewards in rewards_all_batch]\n",
    "    flat_rewards = np.concatenate(discounted_all_batch)\n",
    "    mean_reward = flat_rewards.mean()\n",
    "    std_reward  = flat_rewards.std()\n",
    "    return [(discounted_rewards - mean_reward) / std_reward for discounted_rewards in discounted_all_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-22 -40 -50]\n",
      "[array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(discount_rewards(rewards=[10,0,-50], discount_factor=0.8))\n",
    "print(normalize_discounted_rewards(rewards_all_batch=[[10,0,-50], [10,20]], discount_factor=0.8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_file_path):\n",
    "        \"\"\"\n",
    "            Function to save model's parameters\n",
    "        \"\"\"\n",
    "        torch.save(copy.deepcopy(model.state_dict()), model_file_path)\n",
    "\n",
    "def load_model(model, model_file_path):\n",
    "    \"\"\"\n",
    "        Function to load model's parameters\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(model_file_path))\n",
    "    model.eval()\n",
    "\n",
    "def init_weights(component):\n",
    "    if type(component) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(component.weight)\n",
    "        component.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_reg_training(n_iter, n_episodes_per_update, n_max_steps, discount_factor,\n",
    "                        policy, loss_fn, optimizer, device,\n",
    "                        policy_file_path, best_policy_file_path):\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        policy = policy.to(device)\n",
    "\n",
    "    policy.train()\n",
    "    \n",
    "    best_mean_reward = 0.0\n",
    "\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        rewards_all_batch, grads_all_batch = run_episodes_before_policy_gradient_update(env, n_episodes_per_update, n_max_steps, policy, loss_fn, optimizer, device, \n",
    "                                                                                        generate_GIF=False)\n",
    "\n",
    "        \n",
    "        mean_reward = sum([sum(rewards) for rewards in rewards_all_batch]) / n_episodes_per_update\n",
    "\n",
    "        # Display mean reward\n",
    "        print(f\"\\rIteration: {i + 1}/{n_iter},\"\n",
    "              f\"mean reward: {mean_reward:.1f}\")\n",
    "        print([sum(rewards) for rewards in rewards_all_batch])\n",
    "        \n",
    "        # save best policy\n",
    "        if mean_reward > best_mean_reward:\n",
    "            save_model(policy, best_policy_file_path)\n",
    "            best_mean_reward = mean_reward\n",
    "\n",
    "        all_norm_rewards = normalize_discounted_rewards(rewards_all_batch, discount_factor)\n",
    "\n",
    "        mean_grads_all_batch = []\n",
    "\n",
    "        for param_idx, params in enumerate(policy.parameters()):\n",
    "\n",
    "            weighted_grads = []\n",
    "\n",
    "            for episode, norm_rewards_few_episodes in enumerate(all_norm_rewards):\n",
    "                for step, norm_reward in enumerate(norm_rewards_few_episodes):\n",
    "\n",
    "                    weighted_grads += [torch.tensor(norm_reward).item() * grads_all_batch[episode][step][param_idx]]\n",
    "\n",
    "            mean_grads = torch.stack(weighted_grads).mean(dim=0)\n",
    "            mean_grads_all_batch.append(mean_grads)\n",
    "\n",
    "            if param_idx == -1:\n",
    "                print(\"WEIGHTED\\n\",weighted_grads)\n",
    "                print(\"MEAN\\n\", mean_grads)\n",
    "\n",
    "        # Compute the gradient\n",
    "        optimizer.zero_grad()\n",
    "        for param, gradient in zip(policy.parameters(), mean_grads_all_batch):\n",
    "            param.grad = gradient\n",
    "        optimizer.step()\n",
    "\n",
    "    # save policy\n",
    "    save_model(policy, policy_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with this policy\n",
    "n_iter = 1000\n",
    "n_episodes_per_update = 2\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "lr_ = 0.0001\n",
    "\n",
    "# 400, 10, 0.99, 0.0001 --> MEDIUM\n",
    "# 300, 10, 0.95, 0.0001 --> BAD\n",
    "# 300, 10, 0.9,  0.0001 --> MEDIUM\n",
    "# 300, 10, 0.9, 0.00001 --> MEDIUM GOOD\n",
    "# 200, 10, 0.85,  0.0001 --> BAD\n",
    "# 200, 10, 0.95,  0.001 --> BAD\n",
    "\n",
    "torch.manual_seed(5)\n",
    "\n",
    "policy = SimpleRegression(4,5,1)\n",
    "policy.apply(init_weights)\n",
    "\n",
    "optim = torch.optim.NAdam(policy.parameters(), lr=lr_)\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "policy_file_path = \"./saving/simple_rep_policy.pt\"\n",
    "best_policy_file_path = \"./saving/best_policy.pt\"\n",
    "\n",
    "# simple_reg_training(n_iter, n_episodes_per_update, n_max_steps, discount_factor,\n",
    "#                     policy, loss_fn, optim, device,\n",
    "#                     policy_file_path, best_policy_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episodes_frames_neural_net(env, n_episodes, n_max_steps, policy):\n",
    "\n",
    "    frames_all_episodes = []\n",
    "    \n",
    "    policy.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "\n",
    "            obs, _ = env.reset(seed=episode)\n",
    "            running_reward = 0\n",
    "\n",
    "            # Collect init image\n",
    "            init_frame = env.render()\n",
    "            add_episode_label_on_img(init_frame, episode_num=episode)\n",
    "            frames_all_episodes.append(init_frame)\n",
    "\n",
    "            for step in range(n_max_steps):\n",
    "\n",
    "                obs_tensor = torch.tensor(obs).unsqueeze(0).to(device)                \n",
    "                right_proba = policy(obs_tensor)\n",
    "                # Choose False with probability \n",
    "                action = torch.randn(right_proba.shape).to(device) > right_proba\n",
    "                action = torch.ones(right_proba.shape).to(device) - action.float()\n",
    "                action = int(action.item())\n",
    "                obs, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "                # Collect image\n",
    "                frame = env.render()\n",
    "                add_episode_label_on_img(frame, episode)\n",
    "                frames_all_episodes.append(frame)\n",
    "\n",
    "                running_reward += reward\n",
    "\n",
    "                if truncated or done:\n",
    "\n",
    "                    last_frame = add_reward_label_on_img(frame, running_reward)\n",
    "\n",
    "                    for i in range(20):\n",
    "                        frames_all_episodes.append(last_frame)\n",
    "                        \n",
    "                    break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return frames_all_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "env         = gym.make (\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "n_episodes  = 15\n",
    "n_max_steps = 200\n",
    "\n",
    "# load best policy\n",
    "best_policy = SimpleRegression(4,5,1)\n",
    "best_policy_file_path = \"./saving/simple_rep_policy.pt\"\n",
    "load_model(best_policy, best_policy_file_path)\n",
    "best_policy = best_policy.to(device)\n",
    "\n",
    "#frame_all_episodes = get_episodes_frames_neural_net(env, n_episodes, n_max_steps, best_policy)\n",
    "#imageio.mimwrite(os.path.join('./videos/', 'best_pg_agent_15_episodes.gif'), frame_all_episodes, fps=60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Learning (DQN)\n",
    "\n",
    "Bellman's equations - optimal state value $V^*$ and optimal state action value $Q^*$ - are used to know which state is optimal and which action is optimal given the agent being in a certain state. They are defined as follow: \n",
    "$$\n",
    "\\forall s, V^*(s) = \\max_a ( \\sum_{s'} T(s,a,s') R(s,a,s') + \\gamma \\cdot V^*(s') ) \\\\\n",
    "\\forall (s,a), Q^*(s,a) = \\max_a ( \\sum_{s'} T(s,a,s') R(s,a,s') + \\gamma \\cdot S^*(s',a))\n",
    "$$\n",
    "Several algorithms can be used to find the values of the optimal V-value and Q-value: \n",
    "> 1. V-value and Q-value iteration when the number of states and action is finished (and the set isn't too big) and they're all known;  \n",
    "> 2. Temporal difference learning (TD learning) when partial knowledge of the MDP (high temporal complexity) (*on-policy*);  \n",
    "> 3. Q-learning when partial knowledge of the MDP by randomly exploring the states and actions (high temporal complexity) (*off-policy*);  \n",
    "\n",
    "Considering the complexity of our problem, we'll use a Deep Q-network.  \n",
    "In this case, the function objective will be \n",
    "$$\n",
    "    y(s,a) = r + \\gamma \\cdot max_{a'}  Q_{theta}(s', a')\n",
    "$$\n",
    "to obtain the policy where $\\pi (s) = \\argmax_{a} y(s,a)$. Also, we'll use an $\\epsilon$-greedy policy to explore some behaviour and find some states that might not be find else."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs, n_hidden_units, n_ouputs, device) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "                        nn.Linear(n_inputs, n_hidden_units),\n",
    "                        nn.ELU(),\n",
    "                        nn.Linear(n_hidden_units, n_hidden_units),\n",
    "                        nn.ELU(),\n",
    "                        nn.Linear(n_hidden_units, n_ouputs)\n",
    "                    )\n",
    "        \n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "vanilla_DQN                              [1, 2]                    --\n",
       "├─Sequential: 1-1                        [1, 2]                    --\n",
       "│    └─Linear: 2-1                       [1, 32]                   160\n",
       "│    └─ELU: 2-2                          [1, 32]                   --\n",
       "│    └─Linear: 2-3                       [1, 32]                   1,056\n",
       "│    └─ELU: 2-4                          [1, 32]                   --\n",
       "│    └─Linear: 2-5                       [1, 2]                    66\n",
       "==========================================================================================\n",
       "Total params: 1,282\n",
       "Trainable params: 1,282\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = vanilla_DQN(4, 32, 2, 'cuda:0')\n",
    "summary(dqn, (1,4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# queue elements can be added/removed from both ends\n",
    "from collections import deque "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, model, n_outputs, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        state_tens = torch.tensor(state).unsqueeze(0).to(model.device)\n",
    "        Q_values = model(state_tens)[0]\n",
    "        #print(\"Q_values\", Q_values)\n",
    "        return Q_values.argmax().item()\n",
    "    \n",
    "def sample_experience(replay_buffer, batch_size, device):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch   = [replay_buffer[idx] for idx in indices]\n",
    "    res = [np.array([experience[field_idx] for experience in batch])\n",
    "                for field_idx in range(6)\n",
    "           ] \n",
    "    #print(\"res\", len(res))\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, model, n_outputs, epsilon, replay_buffer):\n",
    "    action = epsilon_greedy_policy(state, model, n_outputs, epsilon)\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
    "    return next_state, reward, done, truncated, info\n",
    "\n",
    "def training_step(model,  replay_buffer, \n",
    "                  batch_size, n_outputs, discount_factor, \n",
    "                  loss_fn, optimizer, \n",
    "                  device):\n",
    "\n",
    "    experiences = sample_experience(replay_buffer, batch_size, device)\n",
    "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
    "    #print(\"Reward: \", len(rewards))\n",
    "    next_states_tens = torch.from_numpy(next_states).to(device)\n",
    "    next_Q_values = model(next_states_tens)\n",
    "    #print(next_Q_values)\n",
    "    max_next_Q_values = torch.max(next_Q_values, dim=1).values\n",
    "    #print(max_next_Q_values)\n",
    "    runs = 1.0 - (dones | truncateds)\n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values.detach().cpu().numpy()\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    target_Q_values = torch.tensor(target_Q_values).float().to(device)\n",
    "    mask = torch.nn.functional.one_hot(torch.from_numpy(actions), num_classes=n_outputs).to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    states_tens = torch.from_numpy(states).to(device)\n",
    "    all_Q_values = model(states_tens)\n",
    "    Q_values = torch.sum(all_Q_values * mask, axis=1).reshape(-1,1)\n",
    "    #print(Q_values.shape)\n",
    "    #print(\"Q_values\", Q_values)\n",
    "    #print(target_Q_values.shape)\n",
    "    loss = loss_fn(target_Q_values, Q_values)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def training(env, n_episodes, max_steps,\n",
    "             model,  replay_buffer, \n",
    "             batch_size, n_outputs, discount_factor, \n",
    "             loss_fn, optimizer, \n",
    "             device):\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        for step in range(max_steps):\n",
    "            # exploration slowly decreasing\n",
    "            epsilon = max(1 - episode / 500, 0.01)\n",
    "            obs, reward, done, truncated, _ = play_one_step(env, obs, model, n_outputs, epsilon, replay_buffer)\n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        if episode > 50:\n",
    "            training_step(model,  replay_buffer, \n",
    "                          batch_size, n_outputs, discount_factor, \n",
    "                          loss_fn, optimizer, \n",
    "                          device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make (\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "n_episodes = 600\n",
    "max_steps = 200\n",
    "model = dqn\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "batch_size = 32\n",
    "n_outputs = 2\n",
    "discount_factor = 0.01\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "device = 'cuda'\n",
    "\n",
    "training(env, n_episodes, max_steps,\n",
    "         model,  replay_buffer, \n",
    "         batch_size, n_outputs, discount_factor, \n",
    "         loss_fn, optimizer, \n",
    "         device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8cc5b7cf7ab45a4eb9c5d10fcde61976ab495d4e3d71a8f87de6440d779c3fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
